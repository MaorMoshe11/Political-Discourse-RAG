{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z9OiauFJRlp7",
        "outputId": "449fe1b6-5a30-4d0b-e484-69c96474d97b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "FAISS index loaded: 2604 vectors\n",
            "Metadata loaded: 2604 rows\n",
            "Differences table loaded: 63 rows\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/63 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 1/63: מטרת החוק\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  2%|▏         | 1/63 [00:00<00:51,  1.21it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 2/63: הגדרות\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  3%|▎         | 2/63 [00:01<00:29,  2.05it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 3/63: רישוי - איסור פעולות\n",
            "Processing section 4/63: רישוי - מתן רישיון\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|▉         | 6/63 [00:01<00:10,  5.60it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 5/63: רישוי - הוראות רישיון\n",
            "Processing section 6/63: רישיון - סייגים למתן רישיון\n",
            "Processing section 7/63: רישוי - תקנות לעניין רישיון\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 13%|█▎        | 8/63 [00:01<00:09,  5.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 8/63: הגבלת רשיון, התלייתו, ביטולו או שינויו\n",
            "Processing section 9/63: רישוי - ביצוע על ידי המנהל\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 14%|█▍        | 9/63 [00:02<00:09,  5.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 10/63: רישוי - הגבלות על העברת רשיון, שיעבוד, עיקול או שינוי מבנה בבעל רשיון\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 17%|█▋        | 11/63 [00:02<00:09,  5.35it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 11/63: רישוי - העברת שליטה\n",
            "Processing section 12/63: רישוי - החזקת אמצעי שליטה בספק שירות חיוני\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 21%|██        | 13/63 [00:02<00:08,  6.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 13/63: רישוי - הגבלות על ספק שירות חיוני\n",
            "Processing section 14/63: רישוי - פעולות בניגוד להגבלות\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 25%|██▌       | 16/63 [00:02<00:05,  7.99it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 15/63: פרק ג' ­ בעל רשיון ספק שירות חיוני\n",
            "Processing section 16/63: חובות בעל רישיון ספק שירות חיוני\n",
            "Processing section 17/63: תחולת החוק על פעולות ספק שירות חיוני\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 27%|██▋       | 17/63 [00:03<00:06,  7.43it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 18/63: תכנית פיתוח\n",
            "Processing section 19/63: עסקאות בעל רשיון ספק חיוני או שירות לאחר\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 33%|███▎      | 21/63 [00:03<00:05,  7.81it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 20/63: פרק ד' - בעל רישיון ייצור\n",
            "Processing section 21/63: הרשות לשירותיים צבוריים - חשמל\n",
            "Processing section 22/63: תפקידי הרשות\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 37%|███▋      | 23/63 [00:04<00:05,  6.75it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 23/63: קביעת תעריף חשמל\n",
            "Processing section 24/63: עדכון תעריפים\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 40%|███▉      | 25/63 [00:04<00:04,  7.76it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 25/63: קביעת אמות מידה לרמת שירות\n",
            "Processing section 26/63: יפים\n",
            "Processing section 27/63: חובות רישום חשבונאי\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 46%|████▌     | 29/63 [00:04<00:03,  9.16it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 28/63: דיווח לרשות\n",
            "Processing section 29/63: מינוי הרשות והרכבה\n",
            "Processing section 30/63: חובת גילוי ואיסור התקשרות\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 49%|████▉     | 31/63 [00:04<00:03,  9.85it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 31/63: ישיבת הרשות\n",
            "Processing section 32/63: סדרי עבודת הרשות\n",
            "Processing section 33/63: הוצאות וגמול\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 52%|█████▏    | 33/63 [00:04<00:02, 10.39it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 34/63: החלת דינים על הרשות ופקיעת כהונה\n",
            "Processing section 35/63: הרשות לשירותים ציבוריים - חשמל - מנהל הרשות\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 56%|█████▌    | 35/63 [00:05<00:03,  7.95it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 36/63: מבנה הרשות\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 38/63 [00:05<00:03,  7.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 37/63: הועדה המייעצת לעניין קביעת תעריפים ואמות מידה לרמת שירות\n",
            "Processing section 38/63: פרק ה' ­ סמכות להיכנס למקרקעין לביצוע עבודות\n",
            "Processing section 39/63: הרשאה לביצוע עבודות והקמת מיתקני חשמל על ידי בעל רישיון\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 63%|██████▎   | 40/63 [00:05<00:02,  8.93it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 40/63: סמכות כניסה למקרקעין וביצוע עבודות בהם\n",
            "Processing section 41/63: ידיעה על כוונה להיכנס למקרקעין\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 65%|██████▌   | 41/63 [00:06<00:03,  7.02it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 42/63: ערעור\n",
            "Processing section 43/63: סדרי הדין בערעור\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|██████▉   | 44/63 [00:06<00:02,  6.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 44/63: פיצויים בעד נזק\n",
            "Processing section 45/63: מיתקנים המחוברים למקרקעין\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▍  | 47/63 [00:06<00:02,  7.97it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 46/63: פרק ז' ­ עבירות ועונשין\n",
            "Processing section 47/63: עבירות ועונשין\n",
            "Processing section 48/63: השחתת מתקן חשמל\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 78%|███████▊  | 49/63 [00:07<00:01,  9.08it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 49/63: עשיית פעולות ללא רישיון\n",
            "Processing section 50/63: סמכויות לדרוש ידיעות ומסמכים\n",
            "Processing section 51/63: הפרעה לבעל רישיון\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 84%|████████▍ | 53/63 [00:07<00:00, 10.42it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 52/63: פרק ח' ­ הוראות שונות\n",
            "Processing section 53/63: מינוי מנהל\n",
            "Processing section 54/63: שעת חירום\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 87%|████████▋ | 55/63 [00:07<00:00, 10.79it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 55/63: שמירת דינים\n",
            "Processing section 56/63: הוראות מעבר\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 90%|█████████ | 57/63 [00:07<00:00,  9.64it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 57/63: שמירת התחייבויות וזכויות\n",
            "Processing section 58/63: העברת זכויות ונכסים\n",
            "Processing section 59/63: ביצוע כללים ותקנות\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 97%|█████████▋| 61/63 [00:08<00:00, 10.30it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 60/63: תיקון חוק החברות הממשלתיות\n",
            "Processing section 61/63: תיקון חוק העבירות המינהליות ­ מס' 3\n",
            "Processing section 62/63: תיקון חוק שירות המדינה (מינויים) ­ מס' 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 63/63 [00:08<00:00,  7.51it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing section 63/63: תחילה\n",
            "Completed.\n",
            "Saved to: /content/drive/MyDrive/CIDR Projects/PoliticalAnalysis/חוק משק החשמל/RAG/electricity_law_diff_debates_report.json\n"
          ]
        }
      ],
      "source": [
        "# ================================\n",
        "# 1) Imports and Setup\n",
        "# ================================\n",
        "!pip install -q transformers accelerate faiss-cpu\n",
        "\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForCausalLM\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# ================================\n",
        "# 2) Configuration\n",
        "# ================================\n",
        "BASE_DIR = \"/content/drive/MyDrive/CIDR Projects/PoliticalAnalysis/חוק משק החשמל/RAG\"\n",
        "INDEX_PATH = os.path.join(BASE_DIR, \"all_protocols_parsed_by_speaker.faiss\")\n",
        "META_PATH  = os.path.join(BASE_DIR, \"all_protocols_parsed_by_speaker_metadata.parquet\")\n",
        "DIFF_PATH  = os.path.join(BASE_DIR, \"cotent_offer_and_final_format_comparison_annotated - cotent_offer_and_final_format_comparison.csv\")\n",
        "\n",
        "OUTPUT_JSON = os.path.join(BASE_DIR, \"electricity_law_diff_debates_report.json\")\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 3) Load Data and Models\n",
        "# ================================\n",
        "index = faiss.read_index(INDEX_PATH)\n",
        "df_meta = pd.read_parquet(META_PATH)\n",
        "df_diff = pd.read_csv(DIFF_PATH)\n",
        "\n",
        "print(f\"FAISS index loaded: {index.ntotal} vectors\")\n",
        "print(f\"Metadata loaded: {len(df_meta)} rows\")\n",
        "print(f\"Differences table loaded: {len(df_diff)} rows\")\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 4) Embedding Model (Qwen Encoder)\n",
        "# ================================\n",
        "ENCODER_MODEL = \"Qwen/Qwen3-Embedding-0.6B\"\n",
        "\n",
        "encoder_tokenizer = AutoTokenizer.from_pretrained(ENCODER_MODEL)\n",
        "encoder_model = AutoModel.from_pretrained(\n",
        "    ENCODER_MODEL,\n",
        "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32\n",
        ").to(device)\n",
        "encoder_model.eval()\n",
        "\n",
        "\n",
        "def encode_texts(texts, max_length=512, batch_size=16):\n",
        "    \"\"\"Returns numpy embeddings for a list of texts.\"\"\"\n",
        "    vectors = []\n",
        "\n",
        "    for i in range(0, len(texts), batch_size):\n",
        "        batch = texts[i:i+batch_size]\n",
        "        tokens = encoder_tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=max_length).to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = encoder_model(**tokens).last_hidden_state\n",
        "            mask = tokens[\"attention_mask\"].unsqueeze(-1)\n",
        "            pooled = (output * mask).sum(dim=1) / mask.sum(dim=1).clamp(min=1e-9)\n",
        "\n",
        "        vectors.append(pooled.cpu().numpy())\n",
        "\n",
        "    return np.vstack(vectors)\n",
        "\n",
        "\n",
        "def embed_single(text):\n",
        "    \"\"\"Convenience wrapper for a single text embedding.\"\"\"\n",
        "    return encode_texts([text])\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 5) FAISS Search Utilities\n",
        "# ================================\n",
        "def search(query, k, index_obj, metadata_df):\n",
        "    \"\"\"Returns top-k matches with scores and associated metadata.\"\"\"\n",
        "    q_vec = embed_single(query)\n",
        "    scores, idxs = index_obj.search(q_vec, k)\n",
        "    hits = metadata_df.iloc[idxs[0]].copy()\n",
        "    hits[\"score\"] = scores[0]\n",
        "    return hits.sort_values(\"score\", ascending=False)\n",
        "\n",
        "\n",
        "def get_context(hit, df, window=5):\n",
        "    \"\"\"Returns contextual transcript window around a retrieved line.\"\"\"\n",
        "    condition = (\n",
        "        (df[\"protocol_number\"] == hit[\"protocol_number\"]) &\n",
        "        (df[\"utterance_idx\"].between(hit[\"utterance_idx\"] - window, hit[\"utterance_idx\"] + window))\n",
        "    )\n",
        "    ctx = df.loc[condition].sort_values([\"protocol_number\", \"utterance_idx\"])\n",
        "    lines = []\n",
        "\n",
        "    for _, r in ctx.iterrows():\n",
        "        sp = str(r.get(\"speaker\", \"\")).strip()\n",
        "        txt = str(r.get(\"text\", \"\")).strip()\n",
        "        lines.append(f\"{sp}: {txt}\" if sp else txt)\n",
        "\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 6) Retrieve up to 5 debates per difference\n",
        "# ================================\n",
        "import pandas as pd\n",
        "def retrieve_debates(\n",
        "    row,\n",
        "    diff_index,\n",
        "    index,\n",
        "    metadata_df,\n",
        "    max_search_hits=200,   # כמה מועמדים למשוך מ-FAISS\n",
        "    top_k=20,              # כמה דיונים לשמור בפועל\n",
        "    min_score=0.85,\n",
        "    window=5\n",
        "):\n",
        "    \"\"\"\n",
        "    Retrieve top-K debates per difference, ranked by normalized similarity.\n",
        "    \"\"\"\n",
        "\n",
        "    # ----------------------------\n",
        "    # 1. Validate difference text\n",
        "    # ----------------------------\n",
        "    diff_raw = row.get(f\"הבדל{diff_index}\", None)\n",
        "\n",
        "    if diff_raw is None or pd.isna(diff_raw):\n",
        "        return [], {\"hits_total\": 0, \"above_threshold\": 0, \"top_raw\": None}\n",
        "\n",
        "    diff_text = str(diff_raw).strip()\n",
        "    if diff_text.lower() in [\"\", \"nan\", \"none\"]:\n",
        "        return [], {\"hits_total\": 0, \"above_threshold\": 0, \"top_raw\": None}\n",
        "\n",
        "    section_title = str(row.get(\"שם סעיף\", \"\")).strip()\n",
        "\n",
        "    # ----------------------------\n",
        "    # 2. FAISS search\n",
        "    # ----------------------------\n",
        "    query = (\n",
        "        f\"דיון בעניין סעיף {section_title}. \"\n",
        "        f\"הפער בין נוסח ההצעה לחוק לבין החוק הסופי הוא: {diff_text}\"\n",
        "    )\n",
        "\n",
        "    hits = search(query, max_search_hits, index, metadata_df)\n",
        "\n",
        "    if hits.empty:\n",
        "        return [], {\"hits_total\": 0, \"above_threshold\": 0, \"top_raw\": None}\n",
        "\n",
        "    # ----------------------------\n",
        "    # 3. Normalize scores\n",
        "    # ----------------------------\n",
        "    top_raw = float(hits[\"score\"].max())\n",
        "    hits[\"score_norm\"] = hits[\"score\"] / top_raw if top_raw > 0 else 0.0\n",
        "\n",
        "    valid_hits = hits[hits[\"score_norm\"] >= min_score]\n",
        "\n",
        "    # ----------------------------\n",
        "    # 4. Take TOP-K highest matches\n",
        "    # ----------------------------\n",
        "    selected_hits = (\n",
        "        valid_hits\n",
        "        .sort_values(\"score_norm\", ascending=False)\n",
        "        .head(top_k)\n",
        "    )\n",
        "\n",
        "    debates = []\n",
        "\n",
        "    for _, h in selected_hits.iterrows():\n",
        "        debate_uid = f\"{int(h['protocol_number'])}_{int(h['utterance_idx'])}\"\n",
        "\n",
        "        debates.append({\n",
        "            \"debate_uid\": debate_uid,\n",
        "            \"protocol_number\": int(h[\"protocol_number\"]),\n",
        "            \"initiator\": str(h.get(\"speaker\", \"\")).strip(),\n",
        "            \"retrieval_score\": float(h[\"score_norm\"]),\n",
        "            \"transcript\": get_context(h, metadata_df, window),\n",
        "        })\n",
        "\n",
        "    stats = {\n",
        "        \"hits_total\": len(hits),\n",
        "        \"above_threshold\": len(valid_hits),\n",
        "        \"returned\": len(debates),\n",
        "        \"top_raw\": top_raw,\n",
        "        \"min_score\": min_score,\n",
        "    }\n",
        "\n",
        "    return debates, stats\n",
        "\n",
        "\n",
        "\n",
        "# # ================================\n",
        "# # 7) Debate LLM Analysis (quote + stance extraction)\n",
        "# # ================================\n",
        "# LLM_MODEL = \"Qwen/Qwen2-7B-Instruct\"\n",
        "\n",
        "# llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL)\n",
        "# llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "#     LLM_MODEL,\n",
        "#     torch_dtype=torch.float16 if device==\"cuda\" else torch.float32,\n",
        "#     device_map=\"auto\")\n",
        "\n",
        "\n",
        "# def analyze_debate(diff_text, transcript, max_new_tokens=512):\n",
        "#     \"\"\"LLM detects relevance, summarizes and extracts supporters/opposers with quotes.\"\"\"\n",
        "\n",
        "#     speakers = sorted({line.split(\":\")[0].strip() for line in transcript.splitlines() if \":\" in line})\n",
        "#     allowed = \", \".join(speakers) if speakers else \"אין דוברים\"\n",
        "\n",
        "#     prompt = f\"\"\"\n",
        "# אתה מנתח פרוטוקול של ועדת כנסת.\n",
        "\n",
        "# השינוי הנבדק:\n",
        "# \\\"\\\"\\\"{diff_text}\\\"\\\"\\\"\n",
        "\n",
        "# תמלול:\n",
        "# \\\"\\\"\\\"{transcript}\\\"\\\"\\\n",
        "\n",
        "# דוברים מותרים:\n",
        "# {allowed}\n",
        "\n",
        "# משימה:\n",
        "# - קבע האם הדיון קשור לשינוי (גבוה / בינוני / לא קשור)\n",
        "# - אם יש קשר: סכם בקצרה\n",
        "# - זהה תומכים ומתנגדים והשב עם ציטוט מדויק שלהם מהטקסט\n",
        "\n",
        "# החזר אך ורק JSON:\n",
        "\n",
        "# {{\n",
        "#  \"relevance\": \"\",\n",
        "#  \"summary\": \"\",\n",
        "#  \"supporters\": [{{\"speaker\": \"\", \"quote\": \"\"}}],\n",
        "#  \"opponents\":  [{{\"speaker\": \"\", \"quote\": \"\"}}]\n",
        "# }}\n",
        "# \"\"\"\n",
        "#     inputs = llm_tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "#     output = llm_model.generate(**inputs, max_new_tokens=max_new_tokens, do_sample=False)\n",
        "#     text = llm_tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "#     try:\n",
        "#         json_part = text[text.index(\"{\"): text.rindex(\"}\")+1]\n",
        "#         out = json.loads(json_part)\n",
        "#     except:\n",
        "#         return {\"relevance\": \"parse_error\", \"summary\": \"\", \"supporters\": [], \"opponents\": []}\n",
        "\n",
        "#     # filter only allowed speakers\n",
        "#     out[\"supporters\"] = [p for p in out.get(\"supporters\", []) if p.get(\"speaker\") in speakers]\n",
        "#     out[\"opponents\"] = [p for p in out.get(\"opponents\", []) if p.get(\"speaker\") in speakers]\n",
        "\n",
        "#     return out\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 8) Build structured JSON per section\n",
        "# ================================\n",
        "def build_section_json(row, index, metadata_df):\n",
        "    \"\"\"\n",
        "    Build clean JSON structure for a single legal section.\n",
        "    Only includes differences that have at least one valid debate.\n",
        "    \"\"\"\n",
        "\n",
        "    section = {\n",
        "        \"section_name\": str(row.get(\"שם סעיף\", \"\")).strip(),\n",
        "        \"proposal_text\": str(row.get(\"הצעת חוק\", \"\")).strip(),\n",
        "        \"final_text\": str(row.get(\"חוק סופי\", \"\")).strip(),\n",
        "        \"gaps\": []\n",
        "    }\n",
        "\n",
        "    # --------------------------------\n",
        "    # Iterate over possible differences\n",
        "    # --------------------------------\n",
        "    for diff_index in range(1, 5):\n",
        "\n",
        "        debates, stats = retrieve_debates(\n",
        "            row=row,\n",
        "            diff_index=diff_index,\n",
        "            index=index,\n",
        "            metadata_df=metadata_df\n",
        "        )\n",
        "\n",
        "        # --------------------------------\n",
        "        # Invariant: only gaps with debates\n",
        "        # --------------------------------\n",
        "        if len(debates) == 0:\n",
        "            continue\n",
        "\n",
        "        diff_text = str(row.get(f\"הבדל{diff_index}\")).strip()\n",
        "\n",
        "        gap_obj = {\n",
        "            \"gap_index\": diff_index,\n",
        "            \"gap_text\": diff_text,\n",
        "            \"retrieval_stats\": stats,\n",
        "            \"debates\": []\n",
        "        }\n",
        "\n",
        "        for j, d in enumerate(debates, start=1):\n",
        "            gap_obj[\"debates\"].append({\n",
        "                \"debate_index\": j,\n",
        "                \"debate_uid\": d[\"debate_uid\"],\n",
        "                \"protocol_number\": d[\"protocol_number\"],\n",
        "                \"initiator\": d[\"initiator\"],\n",
        "                \"retrieval_score\": d[\"retrieval_score\"],\n",
        "                \"transcript\": d[\"transcript\"]\n",
        "            })\n",
        "\n",
        "        section[\"gaps\"].append(gap_obj)\n",
        "\n",
        "    return section\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ================================\n",
        "# 9) Run over all rows and export JSON\n",
        "# ================================\n",
        "results = []\n",
        "\n",
        "for i, row in tqdm(df_diff.iterrows(), total=len(df_diff)):\n",
        "    print(f\"Processing section {i+1}/{len(df_diff)}:\", row.get(\"שם סעיף\", \"\"))\n",
        "    #esults.append(build_section_json(row))\n",
        "    section_json = build_section_json(row, index, df_meta)\n",
        "    # Keep only sections with at least one gap\n",
        "    if len(section_json[\"gaps\"]) > 0:\n",
        "        results.append(section_json)\n",
        "\n",
        "\n",
        "with open(OUTPUT_JSON, \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(results, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "print(\"Completed.\")\n",
        "print(\"Saved to:\", OUTPUT_JSON)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "\n",
        "BASE_DIR = \"/content/drive/MyDrive/CIDR Projects/PoliticalAnalysis/חוק משק החשמל/RAG\"\n",
        "JSON_PATH = f\"{BASE_DIR}/electricity_law_diff_debates_report.json\"\n",
        "OUTPUT_XLSX = f\"{BASE_DIR}/electricity_law_diff_tagging_template.xlsx\"\n",
        "\n",
        "with open(JSON_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(data)} sections\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VPYVWcSs03Sc",
        "outputId": "b1dcd7fe-6044-4cae-eaf0-0b015a83754b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 62 sections\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for section in data:\n",
        "    section_name = section.get(\"section_name\", \"\").strip()\n",
        "\n",
        "    for gap in section.get(\"gaps\", []):\n",
        "        gap_index = gap.get(\"gap_index\")\n",
        "\n",
        "        row_label = f\"{section_name} | הבדל {gap_index}\"\n",
        "\n",
        "        row = {\n",
        "            \"section_name\": section_name,\n",
        "            \"gap_index\": gap_index,\n",
        "            \"row_label\": row_label\n",
        "        }\n",
        "\n",
        "        # Create empty columns for up to 20 debates\n",
        "        for i in range(1, 21):\n",
        "            row[f\"דיון_{i}\"] = \"\"\n",
        "\n",
        "        rows.append(row)\n",
        "\n",
        "print(f\"Created {len(rows)} gap rows\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PdcxqsFX1vvi",
        "outputId": "4e280504-8908-4cb8-fb27-e6a1e2ba7fd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Created 87 gap rows\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# סדר עמודות יפה\n",
        "ordered_columns = (\n",
        "    [\"section_name\", \"gap_index\", \"row_label\"] +\n",
        "    [f\"דיון_{i}\" for i in range(1, 21)]\n",
        ")\n",
        "\n",
        "df = df[ordered_columns]\n",
        "\n",
        "df.to_excel(OUTPUT_XLSX, index=False)\n",
        "\n",
        "print(\"Saved tagging template to:\")\n",
        "print(OUTPUT_XLSX)\n"
      ],
      "metadata": {
        "id": "0Nbs-CAXGOKY",
        "outputId": "9a99fa7c-5e2f-4aa8-ba4c-adffba66015c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved tagging template to:\n",
            "/content/drive/MyDrive/CIDR Projects/PoliticalAnalysis/חוק משק החשמל/RAG/electricity_law_diff_tagging_template.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Validation - (not ready to use cells, just some scatches i made with Noa)"
      ],
      "metadata": {
        "id": "Qr_CnBzv9V-i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_sections_with_keyword(results, keyword):\n",
        "    matches = []\n",
        "    for i, sec in enumerate(results):\n",
        "        name = sec.get(\"section_name\", \"\")\n",
        "        if keyword in name:\n",
        "            matches.append((i, name))\n",
        "    return matches\n",
        "\n",
        "\n",
        "sections_with_horot = find_sections_with_keyword(results, \"הוראות\")\n",
        "\n",
        "for idx, name in sections_with_horot:\n",
        "    print(f\"{idx:02d} | {name}\")\n"
      ],
      "metadata": {
        "id": "mTtWepwt03Nu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TARGET_SEC_IDX = 54\n"
      ],
      "metadata": {
        "id": "-5QCvvIS03Kc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_retrieved_protocols(section):\n",
        "    found = {}\n",
        "    for gap in section[\"gaps\"]:\n",
        "        for deb in gap[\"debates\"]:\n",
        "            p = deb[\"protocol_number\"]\n",
        "            found.setdefault(p, []).append(deb)\n",
        "    return found\n",
        "\n",
        "\n",
        "sec = results[TARGET_SEC_IDX]\n",
        "retrieved_protocols = extract_retrieved_protocols(sec)\n",
        "\n",
        "print(\"Retrieved protocols:\")\n",
        "for p, debates in sorted(retrieved_protocols.items()):\n",
        "    print(f\"Protocol {p}: {len(debates)} debates\")\n"
      ],
      "metadata": {
        "id": "W-uig2QJ03Hc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "expert_protocols = {\n",
        "    1: [2, 9, 10, 16, 30, 35],\n",
        "    2: [4, 16, 18],\n",
        "    3: [6, 9, 10, 19, 21, 27, 28, 33],\n",
        "    4: [18],\n",
        "    5: [16],\n",
        "    6: [35, 45],\n",
        "    7: [61, 62, 63, 65, 67, 68, 69, 70, 71],\n",
        "    8: [5, 6, 8, 13],\n",
        "}\n",
        "\n",
        "\n",
        "def compare_with_expert(retrieved_protocols, expert_protocols):\n",
        "    rows = []\n",
        "\n",
        "    for proto in sorted(expert_protocols.keys()):\n",
        "        rows.append({\n",
        "            \"protocol\": proto,\n",
        "            \"expert_has_discussion\": True,\n",
        "            \"retrieved\": proto in retrieved_protocols,\n",
        "            \"n_retrieved_debates\": len(retrieved_protocols.get(proto, [])),\n",
        "        })\n",
        "\n",
        "    # false positives (retrieved but expert didn't mention)\n",
        "    for proto in retrieved_protocols:\n",
        "        if proto not in expert_protocols:\n",
        "            rows.append({\n",
        "                \"protocol\": proto,\n",
        "                \"expert_has_discussion\": False,\n",
        "                \"retrieved\": True,\n",
        "                \"n_retrieved_debates\": len(retrieved_protocols.get(proto, [])),\n",
        "            })\n",
        "\n",
        "    return pd.DataFrame(rows).sort_values(\"protocol\")\n",
        "\n",
        "\n",
        "comparison_df = compare_with_expert(retrieved_protocols, expert_protocols)\n",
        "comparison_df\n"
      ],
      "metadata": {
        "id": "cEvdth7i02-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tp = ((comparison_df.expert_has_discussion) & (comparison_df.retrieved)).sum()\n",
        "fn = ((comparison_df.expert_has_discussion) & (~comparison_df.retrieved)).sum()\n",
        "fp = ((~comparison_df.expert_has_discussion) & (comparison_df.retrieved)).sum()\n",
        "\n",
        "precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "\n",
        "print(f\"TP (found & expected): {tp}\")\n",
        "print(f\"FN (missed expert):   {fn}\")\n",
        "print(f\"FP (extra protocols): {fp}\")\n",
        "print(f\"Precision: {precision:.2f}\")\n",
        "print(f\"Recall:    {recall:.2f}\")\n"
      ],
      "metadata": {
        "id": "kfT5hXS62P02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IhR7REip2Pyx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IkicDOUZ2Pwf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3cPQmcRa2PuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_MejZ0pJ2PsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hqzBqbXB2Pp3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UBl6MXyw2PnU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DuaVIpzU2PkH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6y40fM_32PhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oOzYx5zXRq9X"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# If you already have OUTPUT_JSON from the previous code, reuse it\n",
        "OUTPUT_JSON = os.path.join(\n",
        "    BASE_DIR,\n",
        "    \"electricity_law_diff_debates_report.json\"\n",
        ")\n",
        "\n",
        "with open(OUTPUT_JSON, \"r\", encoding=\"utf-8\") as f:\n",
        "    results = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(results)} sections from JSON\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-BAZ4zvASHH2"
      },
      "outputs": [],
      "source": [
        "def build_section_df(results):\n",
        "    \"\"\"\n",
        "    Build a DataFrame with one row per legal section.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "    for sec_idx, sec in enumerate(results):\n",
        "        gaps = sec.get(\"פערים\", []) or []\n",
        "        row = {\n",
        "            \"section_index\": sec_idx,\n",
        "            \"section_name\": sec.get(\"שם סעיף\", \"\"),\n",
        "            \"proposal_text\": sec.get(\"הסעיף לפי נוסח ההצעה\", \"\"),\n",
        "            \"final_text\": sec.get(\"הסעיף בנוסח החוק בפועל\", \"\"),\n",
        "            \"n_gaps\": len(gaps),\n",
        "        }\n",
        "        rows.append(row)\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "section_df = build_section_df(results)\n",
        "section_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X1nSJxhrSKwd"
      },
      "outputs": [],
      "source": [
        "def build_gap_df(results):\n",
        "    \"\"\"\n",
        "    Build a DataFrame with one row per gap (difference) in each section.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for sec_idx, sec in enumerate(results):\n",
        "        section_name = sec.get(\"שם סעיף\", \"\")\n",
        "        gaps = sec.get(\"פערים\", []) or []\n",
        "\n",
        "        for gap in gaps:\n",
        "            stats = gap.get(\"סטטיסטיקה\", {}) or {}\n",
        "            row = {\n",
        "                \"section_index\": sec_idx,\n",
        "                \"section_name\": section_name,\n",
        "                \"gap_index\": gap.get(\"מספר פער\", None),\n",
        "                \"gap_text\": gap.get(\"תיאור הפער\", \"\"),\n",
        "                \"hits_total\": stats.get(\"hits_total\", 0),\n",
        "                \"hits_above_threshold\": stats.get(\"above_threshold\", 0),\n",
        "                \"top_raw_score\": stats.get(\"top_raw\", None),\n",
        "                \"min_score\": stats.get(\"min_score\", None),\n",
        "                \"n_debates\": len(gap.get(\"דיונים\", []) or []),\n",
        "            }\n",
        "            rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "gap_df = build_gap_df(results)\n",
        "gap_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YoP9jqrdSM-7"
      },
      "outputs": [],
      "source": [
        "def build_debate_df(results):\n",
        "    \"\"\"\n",
        "    Build a DataFrame with one row per debate for each gap and section.\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for sec_idx, sec in enumerate(results):\n",
        "        section_name = sec.get(\"שם סעיף\", \"\")\n",
        "        gaps = sec.get(\"פערים\", []) or []\n",
        "\n",
        "        for gap in gaps:\n",
        "            gap_index = gap.get(\"מספר פער\", None)\n",
        "            gap_text = gap.get(\"תיאור הפער\", \"\")\n",
        "            debates = gap.get(\"דיונים\", []) or []\n",
        "\n",
        "            for deb in debates:\n",
        "                row = {\n",
        "                    \"section_index\": sec_idx,\n",
        "                    \"section_name\": section_name,\n",
        "                    \"gap_index\": gap_index,\n",
        "                    \"gap_text\": gap_text,\n",
        "                    \"debate_index\": deb.get(\"מספר דיון\", None),\n",
        "                    \"protocol_number\": deb.get(\"מספר פרוטוקול\", None),\n",
        "                    \"initiator\": deb.get(\"מי יזם את השיח\", \"\"),\n",
        "                    \"retrieval_score\": deb.get(\"ציון\", None),\n",
        "                    \"relevance\": deb.get(\"relevance\", \"\"),\n",
        "                    \"summary\": deb.get(\"summary\", \"\"),\n",
        "                    \"n_supporters\": len(deb.get(\"supporters\", []) or []),\n",
        "                    \"n_opponents\": len(deb.get(\"opponents\", []) or []),\n",
        "                }\n",
        "                rows.append(row)\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "debate_df = build_debate_df(results)\n",
        "debate_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3q-j0AQKSPVj"
      },
      "outputs": [],
      "source": [
        "def build_stance_df(results):\n",
        "    \"\"\"\n",
        "    Build a DataFrame with one row per (speaker, stance) inside each debate.\n",
        "\n",
        "    Columns include:\n",
        "      - section_name, gap_text, debate_index, protocol_number\n",
        "      - stance (\"support\" / \"oppose\")\n",
        "      - speaker, quote\n",
        "      - relevance (of the debate), retrieval_score\n",
        "    \"\"\"\n",
        "    rows = []\n",
        "\n",
        "    for sec_idx, sec in enumerate(results):\n",
        "        section_name = sec.get(\"שם סעיף\", \"\")\n",
        "        gaps = sec.get(\"פערים\", []) or []\n",
        "\n",
        "        for gap in gaps:\n",
        "            gap_index = gap.get(\"מספר פער\", None)\n",
        "            gap_text = gap.get(\"תיאור הפער\", \"\")\n",
        "            debates = gap.get(\"דיונים\", []) or []\n",
        "\n",
        "            for deb in debates:\n",
        "                debate_index = deb.get(\"מספר דיון\", None)\n",
        "                protocol_number = deb.get(\"מספר פרוטוקול\", None)\n",
        "                initiator = deb.get(\"מי יזם את השיח\", \"\")\n",
        "                retrieval_score = deb.get(\"ציון\", None)\n",
        "                relevance = deb.get(\"relevance\", \"\")\n",
        "                summary = deb.get(\"summary\", \"\")\n",
        "\n",
        "                # Supporters\n",
        "                for s in deb.get(\"supporters\", []) or []:\n",
        "                    rows.append({\n",
        "                        \"section_index\": sec_idx,\n",
        "                        \"section_name\": section_name,\n",
        "                        \"gap_index\": gap_index,\n",
        "                        \"gap_text\": gap_text,\n",
        "                        \"debate_index\": debate_index,\n",
        "                        \"protocol_number\": protocol_number,\n",
        "                        \"initiator\": initiator,\n",
        "                        \"stance\": \"support\",\n",
        "                        \"speaker\": s.get(\"speaker\", \"\"),\n",
        "                        \"quote\": s.get(\"quote\", \"\"),\n",
        "                        \"relevance\": relevance,\n",
        "                        \"retrieval_score\": retrieval_score,\n",
        "                        \"debate_summary\": summary,\n",
        "                    })\n",
        "\n",
        "                # Opponents\n",
        "                for o in deb.get(\"opponents\", []) or []:\n",
        "                    rows.append({\n",
        "                        \"section_index\": sec_idx,\n",
        "                        \"section_name\": section_name,\n",
        "                        \"gap_index\": gap_index,\n",
        "                        \"gap_text\": gap_text,\n",
        "                        \"debate_index\": debate_index,\n",
        "                        \"protocol_number\": protocol_number,\n",
        "                        \"initiator\": initiator,\n",
        "                        \"stance\": \"oppose\",\n",
        "                        \"speaker\": o.get(\"speaker\", \"\"),\n",
        "                        \"quote\": o.get(\"quote\", \"\"),\n",
        "                        \"relevance\": relevance,\n",
        "                        \"retrieval_score\": retrieval_score,\n",
        "                        \"debate_summary\": summary,\n",
        "                    })\n",
        "\n",
        "    return pd.DataFrame(rows)\n",
        "\n",
        "stance_df = build_stance_df(results)\n",
        "stance_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjsDw6H_Sq6X"
      },
      "outputs": [],
      "source": [
        "def list_sections(results):\n",
        "    \"\"\"\n",
        "    Prints all section names with index numbers for selection.\n",
        "    \"\"\"\n",
        "    print(\"Available sections:\")\n",
        "    print(\"-\" * 60)\n",
        "    for idx, sec in enumerate(results):\n",
        "        name = sec.get(\"שם סעיף\", \"\").strip()\n",
        "        print(f\"{idx:02d} | {name}\")\n",
        "    print(\"-\" * 60)\n",
        "\n",
        "list_sections(results)\n",
        "\n",
        "def get_section_by_index(results, index):\n",
        "    \"\"\"\n",
        "    Returns a single section JSON object by numeric index.\n",
        "    \"\"\"\n",
        "    if index < 0 or index >= len(results):\n",
        "        raise ValueError(f\"Section index {index} is out of range (0-{len(results)-1}).\")\n",
        "    return results[index]\n",
        "\n",
        "\n",
        "def pretty_print_section(section):\n",
        "    \"\"\"\n",
        "    Displays a human-readable preview summary of a section.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"SECTION SELECTED\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"Name: {section.get('שם סעיף', '').strip()}\")\n",
        "    print(f\"Gaps found: {len(section.get('פערים', []))}\")\n",
        "    print(\"-\" * 80)\n",
        "\n",
        "    print(\"\\nProposal Version:\")\n",
        "    print(section.get(\"הסעיף לפי נוסח ההצעה\", \"\").strip())\n",
        "\n",
        "    print(\"\\nFinal Approved Version:\")\n",
        "    print(section.get(\"הסעיף בנוסח החוק בפועל\", \"\").strip())\n",
        "\n",
        "    print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A0yP8y8hSTVU"
      },
      "outputs": [],
      "source": [
        "# Step 1: show list\n",
        "list_sections(results)\n",
        "\n",
        "# Step 2: choose one manually (example: section #3)\n",
        "chosen_index = 3\n",
        "\n",
        "# Step 3: fetch object\n",
        "selected_section = get_section_by_index(results, chosen_index)\n",
        "\n",
        "# Step 4: display summary\n",
        "pretty_print_section(selected_section)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XBct7lvTNFU"
      },
      "outputs": [],
      "source": [
        "# Run this cell in Colab to choose interactively\n",
        "list_sections(results)\n",
        "chosen_index = int(input(\"\\nEnter section index: \"))\n",
        "selected_section = get_section_by_index(results, chosen_index)\n",
        "pretty_print_section(selected_section)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf830RObSk68"
      },
      "outputs": [],
      "source": [
        "debate_df.to_excel(os.path.join(BASE_DIR, \"debate_level_table.xlsx\"), index=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FAG1gDI0SnGE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}